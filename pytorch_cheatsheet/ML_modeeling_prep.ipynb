{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 5])\n",
      "torch.Size([5, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "#permute\n",
    "x = torch.randn(2, 3, 5)\n",
    "print(x.size())\n",
    "print(torch.permute(x, (2, 0, 1)).size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 8\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#GRADIENT CLIPPING#Batch normalisation can be tricky to implement in recurrent neural networks\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#Instead we might use something lese like gradient clipping. \u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#Used for the exploding gradients problem. \u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#Clip the gradients to be within a certain threshold. \u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[1;32m----> 8\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mparameters, max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;66;03m# Might introducde bvanishing gradient problem\u001b[39;00m\n\u001b[0;32m      9\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_value_(model\u001b[38;5;241m.\u001b[39mparameters(), clip_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "#GRADIENT CLIPPING\n",
    "# #Batch normalisation can be tricky to implement in recurrent neural networks\n",
    "#Instead we might use something lese like gradient clipping. \n",
    "#Used for the exploding gradients problem. \n",
    "#Clip the gradients to be within a certain threshold. \n",
    "\n",
    "from torch import nn\n",
    "\n",
    "torch.nn.utils.clip_grad_norm_(model.parameters, max_norm=1)# Might introducde bvanishing gradient problem\n",
    "torch.nn.utils.clip_grad_value_(model.parameters(), clip_value=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Weight Initialisation\n",
    "#Xavier\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "\n",
    "class Neural_Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(28*28, 512)\n",
    "        self.fc2 = nn.Linear(512, 512)\n",
    "        self.fc3 = nn.Linear(512, 10)\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    def init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            init.xavier_uniform_(m.weight)  # Xavier Uniform Initialization\n",
    "            if m.bias is not None:\n",
    "                init.zeros_(m.bias)  # Initialize biases to 0\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.fc2(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = Neural_Network()\n",
    "\n",
    "#Other variations, Default (random), XAvier (Glorot), He, Normal(Gaussian). Uniform, Bias, prthogonal.  Know when and where to use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Batch Normalisation\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class Neural_Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.Linear_Relu_Unit = nn.Sequential(\n",
    "            nn.Linear(28 * 28 * 1, 512),\n",
    "            nn.BatchNorm1d(512),  # Batch Normalization after the first linear layer\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),  # Dropout to prevent overfitting\n",
    "\n",
    "            nn.Linear(512, 512),\n",
    "            nn.BatchNorm1d(512),  # Batch Normalization after the second linear layer\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.Linear_Relu_Unit(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Layer Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scheduling\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# Define the neural network class (your model)\n",
    "class Neural_Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.Linear_Relu_Unit = nn.Sequential(\n",
    "            nn.Linear(28 * 28 * 1, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.Linear_Relu_Unit(x)\n",
    "        return logits\n",
    "\n",
    "# Define your model, optimizer, and loss function\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = Neural_Network().to(device)\n",
    "optimiser = torch.optim.Adam(params=model.parameters(), lr=0.001)  # Initial learning rate\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define your learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimiser, step_size=5, gamma=0.1)  # Every 5 epochs, lr will be reduced by a factor of 0.1\n",
    "\n",
    "# Define the DataLoader (training loop, for example)\n",
    "# For simplicity, assume you have your train_dataloader, valid_dataloader defined\n",
    "# Note: You should use proper data here\n",
    "train_dataloader = DataLoader(training_data, shuffle=True, batch_size=64)  # Your DataLoader\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimiser, scheduler=None):\n",
    "    model.train()\n",
    "    correct = 0\n",
    "    data_size = len(dataloader.dataset)\n",
    "\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        correct += (pred.argmax(1) == y).sum().item()\n",
    "\n",
    "        optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print(f\"Loss is {loss.item()}\")\n",
    "\n",
    "    # Step the scheduler at the end of the epoch\n",
    "    if scheduler:\n",
    "        scheduler.step()\n",
    "\n",
    "    correct /= data_size\n",
    "    print(f\"Epoch finished with accuracy: {correct * 100:.2f}%\")\n",
    "\n",
    "# Training loop with learning rate scheduler\n",
    "no_epochs = 10  # Number of epochs\n",
    "for epoch in range(no_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{no_epochs}\")\n",
    "    train(train_dataloader, model, loss_fn, optimiser, scheduler)\n",
    "    print(f\"Current Learning Rate: {optimiser.param_groups[0]['lr']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Droput\n",
    "class Neural_Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.Linear_Relu_Unit = nn.Sequential(\n",
    "            nn.Linear(28 * 28 * 1, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),  # Dropout to prevent overfitting\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.Linear_Relu_Unit(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#L1 and #l2. Add additional terms to out loss function to help limit weights size. \n",
    "#Constrain them to imporve genralisation\n",
    "\n",
    "#L1 add absolute values, #l2 ad sum of squares\n",
    "#L2\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.01)  # weight_decay is L2 regularization term\n",
    "\n",
    "#L1\n",
    "lambda_l1 = 0.001  # L1 regularization strength\n",
    "l1_norm = sum(p.abs().sum() for p in model.parameters())  # L1 norm of the model's parameters\n",
    "loss = original_loss + lambda_l1 * l1_norm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_np = np.array(X)\n",
    "y_np = np.array(y)\n",
    "\n",
    "print(X_np.shape)\n",
    "print(y_np.shape)\n",
    "\n",
    "# Flatten data to fit the scaler on entire x and y range\n",
    "X_flattened = X_np.reshape(-1, 2)  # Shape: (num_samples * seq_len, 2)\n",
    "y_flattened = y_np.reshape(-1, 2)  # Shape: (num_samples * pred_len, 2)\n",
    "print(X_flattened.shape)\n",
    "print(y_flattened.shape)\n",
    "scaler = StandardScaler()\n",
    "# Fit the scaler on the entire dataset\n",
    "scaler.fit(np.vstack([X_flattened, y_flattened]))\n",
    "\n",
    "# Transform X and y using the same scaler\n",
    "X_scaled = scaler.transform(X_flattened).reshape(X_np.shape)\n",
    "y_scaled = scaler.transform(y_flattened).reshape(y_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MSE, MAE, Accuracy, Precision/recall, F1-score, AUC-ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SGD, ADAM, RMSProp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Engineering and PRe-processomg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Custom loss funtion\n",
    "#Contrastive loss for metric learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Imbalance Handling\n",
    "#Oversampling, SMOTE. Weighted Loss\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Example: Imbalanced dataset\n",
    "# Let's assume we have a dataset with 100 samples, 80 of the majority class (label 0) and 20 of the minority class (label 1)\n",
    "X = np.random.rand(100, 10)  # 100 samples with 10 features each\n",
    "y = np.array([0] * 80 + [1] * 20)  # Imbalanced labels (80 of class 0, 20 of class 1)\n",
    "\n",
    "# Random Oversampling: increase the minority class\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_resampled, y_resampled = ros.fit_resample(X, y)\n",
    "\n",
    "# Check the new class distribution\n",
    "print(f\"Original class distribution: {np.bincount(y)}\")\n",
    "print(f\"Resampled class distribution: {np.bincount(y_resampled)}\")\n",
    "\n",
    "# Convert resampled data to PyTorch tensors\n",
    "X_resampled_tensor = torch.tensor(X_resampled, dtype=torch.float32)\n",
    "y_resampled_tensor = torch.tensor(y_resampled, dtype=torch.long)\n",
    "\n",
    "# Create DataLoader for the oversampled dataset\n",
    "dataset = TensorDataset(X_resampled_tensor, y_resampled_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Example: Loop through DataLoader\n",
    "for X_batch, y_batch in dataloader:\n",
    "    print(X_batch.shape, y_batch.shape)  # Batch shape for features and labels\n",
    "    break\n",
    "\n",
    "\n",
    "#Smote\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Example: Imbalanced dataset\n",
    "X = np.random.rand(100, 10)  # 100 samples with 10 features each\n",
    "y = np.array([0] * 80 + [1] * 20)  # Imbalanced labels (80 of class 0, 20 of class 1)\n",
    "\n",
    "# SMOTE: Generate synthetic samples for the minority class\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "# Check the new class distribution\n",
    "print(f\"Original class distribution: {np.bincount(y)}\")\n",
    "print(f\"Resampled class distribution: {np.bincount(y_resampled)}\")\n",
    "\n",
    "# Convert resampled data to PyTorch tensors\n",
    "X_resampled_tensor = torch.tensor(X_resampled, dtype=torch.float32)\n",
    "y_resampled_tensor = torch.tensor(y_resampled, dtype=torch.long)\n",
    "\n",
    "# Create DataLoader for the oversampled dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving a model and a tensor\n",
    "#torchvision.utils.save_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ”¥ Curveball Challenge (10 min)\n",
    "ðŸ’¡ Twist:\n",
    "Your manager asks you to quantize the model to reduce memory usage while keeping performance high.\n",
    "\n",
    "ðŸ‘‰ Modify your model to use torch.quantization to make it lightweight.\n",
    "\n",
    "ðŸ›  Tasks:\n",
    "\n",
    "Apply dynamic quantization to reduce memory usage.\n",
    "Show how you would convert the model to a quantized version in PyTorch.\n",
    "ðŸš€ How would you do this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.quantization\n",
    "\n",
    "# Apply dynamic quantization to the Linear layers\n",
    "quantized_model = torch.quantization.quantize_dynamic(\n",
    "    model,  # Your trained model\n",
    "    {torch.nn.Linear},  # Layers to quantize\n",
    "    dtype=torch.qint8  # Convert weights to int8\n",
    ")\n",
    "\n",
    "quantized_model.to(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 40\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished running basic DDP example on rank \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrank\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 40\u001b[0m     \u001b[43mdemo_basic\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 20\u001b[0m, in \u001b[0;36mdemo_basic\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdemo_basic\u001b[39m():\n\u001b[1;32m---> 20\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mset_device(\u001b[38;5;28mint\u001b[39m(\u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLOCAL_RANK\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n\u001b[0;32m     21\u001b[0m     dist\u001b[38;5;241m.\u001b[39minit_process_group(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnccl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     22\u001b[0m     rank \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39mget_rank()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "#Distributed training\n",
    "import os\n",
    "import sys\n",
    "import tempfile\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "def setup(rank, world_size):\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12355'\n",
    "\n",
    "    # initialize the process group\n",
    "    dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)\n",
    "\n",
    "def cleanup():\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "class ToyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ToyModel, self).__init__()\n",
    "        self.net1 = nn.Linear(10, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.net2 = nn.Linear(10, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net2(self.relu(self.net1(x)))\n",
    "\n",
    "\n",
    "def demo_basic(rank, world_size):\n",
    "    print(f\"Running basic DDP example on rank {rank}.\")\n",
    "    setup(rank, world_size)\n",
    "\n",
    "    # create model and move it to GPU with id rank\n",
    "    model = ToyModel().to(rank)\n",
    "    ddp_model = DDP(model, device_ids=[rank])\n",
    "\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    outputs = ddp_model(torch.randn(20, 10))\n",
    "    labels = torch.randn(20, 5).to(rank)\n",
    "    loss_fn(outputs, labels).backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    cleanup()\n",
    "    print(f\"Finished running basic DDP example on rank {rank}.\")\n",
    "\n",
    "\n",
    "def run_demo(demo_fn, world_size):\n",
    "    mp.spawn(demo_fn,\n",
    "             args=(world_size,),\n",
    "             nprocs=world_size,\n",
    "             join=True)\n",
    "    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
